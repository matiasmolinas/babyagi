# Technical Reference: Extending BabyAGI Functions with LLM-Generated Code in a Python Sandbox

This document details the technical process of extending BabyAGI's functionality by incorporating functions generated by a Large Language Model (LLM) within a secure Python-based sandbox environment. It focuses on:

1. **Function Generation from Description:** Process of converting a function description into executable code using an LLM, while respecting imposed restrictions.
2. **Python Sandbox Design:** Architecture and implementation details of the Python-based sandbox environment used to execute the LLM-generated code securely.

## 1. Function Generation from Description

### 1.1 Process Overview

1. **Function Description Input:** A user provides a natural language description of the desired function, outlining its purpose, input parameters, and expected output.
2. **Library Restrictions Input:** A set of allowed libraries for the sandbox is specified, ensuring controlled access to external resources.
3. **Prompt Generation:** BabyAGI constructs a prompt for the LLM, incorporating the function description, allowed libraries, and additional constraints.
4. **Code Generation:** The LLM generates Python code based on the provided prompt.
5. **Code Validation:** BabyAGI performs basic code validation to check for syntax errors and adherence to allowed libraries.
6. **Sandbox Execution:** The generated code is executed within the Python-based sandbox environment, which monitors resource usage and restricts access to disallowed functionalities.
7. **Output & Validation:** The output of the function execution is captured, validated against expected results, and returned.
8. **Function Registration (Optional):** If the generated function is deemed valuable, it can be registered within BabyAGI's function database for future use.

### 1.2 Advanced Prompt Engineering

To enhance the quality and security of generated code, we'll employ advanced prompt engineering techniques:

1. **Role and Context Setting:**
   - Assign a role to the LLM (e.g., "Expert Python Developer")
   - Provide context about the secure environment

2. **Explicit Constraints:**
   - Clearly state allowed libraries and forbidden operations
   - Specify coding style and error handling requirements

3. **Few-Shot Learning:**
   - Include examples of properly formatted and secure functions

4. **Output Structuring:**
   - Request specific sections (imports, main function, helper functions)

5. **Reflection and Self-Correction:**
   - Ask the LLM to review and improve its own code

6. **Safety Checks:**
   - Instruct the LLM to add comments explaining security considerations

### 1.3 Example Improved Prompt

```
You are an Expert Python Developer tasked with creating a secure function for BabyAGI, a system that runs code in a restricted sandbox environment. Your goal is to write efficient, secure, and well-documented code.

Task: Write a Python function that calculates the factorial of a given number.

Constraints:
1. Use only the 'math' library. No other imports are allowed.
2. The function must take an integer as input and return an integer.
3. Handle cases where the input is negative or zero.
4. Include error handling and input validation.
5. Follow PEP 8 style guidelines.
6. Add comments explaining any security considerations.

Function Signature:
def calculate_factorial(n: int) -> int:

Example of a secure function adhering to these constraints:

```python
import math

def is_prime(n: int) -> bool:
    """
    Check if a number is prime.
    
    Args:
        n (int): The number to check.
    
    Returns:
        bool: True if the number is prime, False otherwise.
    
    Raises:
        ValueError: If the input is not a positive integer.
    """
    if not isinstance(n, int) or n < 2:
        raise ValueError("Input must be a positive integer greater than 1.")
    
    # Optimization: Check up to the square root of n
    for i in range(2, int(math.sqrt(n)) + 1):
        if n % i == 0:
            return False
    return True

# Security note: This function only uses the math library and
# performs basic arithmetic operations, minimizing potential risks.
```

Now, please write the calculate_factorial function following a similar structure and adhering to the given constraints. After writing the function, review your code for any potential security issues or improvements, and add comments explaining your considerations.
```

## 2. Python Sandbox Design

### 2.1 Architecture

- **Process Isolation:** Use Python's `multiprocessing` module to run code in a separate process.
- **Resource Limits:** Implement CPU time limits, memory usage restrictions, and disk I/O constraints.
- **Library Restrictions:** Use a custom import hook to control which modules can be imported.
- **Filesystem Restrictions:** Implement a custom file system wrapper to limit read/write access.
- **Restricted Builtins:** Create a restricted set of Python built-in functions and types.
- **Code Analysis:** Use the `ast` module to perform static analysis on the code before execution.

### 2.2 Implementation

#### 2.2.1 Process Isolation and Resource Limits

```python
import multiprocessing
import resource
import signal

def limit_resources():
    # Set CPU time limit (in seconds)
    resource.setrlimit(resource.RLIMIT_CPU, (10, 10))
    # Set memory limit (in bytes)
    resource.setrlimit(resource.RLIMIT_AS, (100 * 1024 * 1024, 100 * 1024 * 1024))

def run_in_sandbox(code):
    def executer():
        limit_resources()
        exec(code)

    process = multiprocessing.Process(target=executer)
    process.start()
    process.join(timeout=10)  # Timeout after 10 seconds
    if process.is_alive():
        process.terminate()
        raise TimeoutError("Execution timed out")
```

#### 2.2.2 Library Restrictions

```python
import sys
from importlib.abc import MetaPathFinder, Loader

class RestrictedImporter(MetaPathFinder, Loader):
    def __init__(self, allowed_modules):
        self.allowed_modules = allowed_modules

    def find_spec(self, fullname, path, target=None):
        if fullname in self.allowed_modules:
            return importlib.util.spec_from_loader(fullname, self)
        return None

    def load_module(self, fullname):
        if fullname in self.allowed_modules:
            return importlib.import_module(fullname)
        raise ImportError(f"Import of '{fullname}' is not allowed")

def setup_restricted_imports(allowed_modules):
    restricted_importer = RestrictedImporter(allowed_modules)
    sys.meta_path.insert(0, restricted_importer)
```

#### 2.2.3 Filesystem Restrictions

```python
import os

class RestrictedFileSystem:
    def __init__(self, allowed_paths):
        self.allowed_paths = allowed_paths

    def check_path(self, path):
        return any(os.path.commonpath([path, allowed]) == allowed for allowed in self.allowed_paths)

    def open(self, file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None):
        if not self.check_path(file):
            raise PermissionError(f"Access to '{file}' is not allowed")
        return open(file, mode, buffering, encoding, errors, newline, closefd, opener)

# Replace built-in open function
__builtins__['open'] = RestrictedFileSystem(['/allowed/path']).open
```

#### 2.2.4 Restricted Builtins

```python
safe_builtins = {
    'abs', 'all', 'any', 'ascii', 'bin', 'bool', 'bytearray', 'bytes', 'callable',
    'chr', 'complex', 'divmod', 'enumerate', 'filter', 'float', 'format', 'frozenset',
    'hash', 'hex', 'int', 'isinstance', 'issubclass', 'iter', 'len', 'list', 'map',
    'max', 'min', 'next', 'oct', 'ord', 'pow', 'range', 'repr', 'reversed', 'round',
    'set', 'slice', 'sorted', 'str', 'sum', 'tuple', 'type', 'zip'
}

restricted_globals = {name: __builtins__[name] for name in safe_builtins}
```

#### 2.2.5 Code Analysis

```python
import ast

def analyze_code(code):
    tree = ast.parse(code)
    for node in ast.walk(tree):
        if isinstance(node, (ast.Import, ast.ImportFrom)):
            for alias in node.names:
                if alias.name not in allowed_modules:
                    raise SecurityError(f"Import of '{alias.name}' is not allowed")
        elif isinstance(node, ast.Call):
            if isinstance(node.func, ast.Attribute):
                if node.func.attr in ['exec', 'eval', 'compile']:
                    raise SecurityError(f"Use of '{node.func.attr}' is not allowed")
    return True
```

### 2.3 Integration with BabyAGI

1. **Sandbox Module:** Create a new module `sandbox.py` that encapsulates all the sandbox functionality.
2. **Function Generation Pipeline:**
   - Generate code using the improved prompt
   - Analyze the generated code using `analyze_code()`
   - Set up the restricted environment (imports, builtins, filesystem)
   - Execute the code using `run_in_sandbox()`
3. **Error Handling:** Implement comprehensive error handling to catch and report any security violations or execution errors.
4. **Logging:** Add detailed logging throughout the sandbox execution process for auditing and debugging purposes.

### 2.4 Security Considerations

- Regularly update the list of allowed modules and safe builtins based on security assessments.
- Implement a comprehensive test suite to verify sandbox integrity and catch potential vulnerabilities.
- Consider using static analysis tools to further enhance code security checks.
- Regularly review and update resource limits based on performance metrics and security requirements.

## 3. Conclusion

By implementing a Python-based sandbox environment, BabyAGI can safely leverage the power of LLMs for generating and executing new functions. This approach extends the framework's capabilities while mitigating potential security risks associated with executing code from untrusted sources. The advanced prompt engineering techniques ensure higher quality and more secure code generation. Continuous refinement of the sandbox design, prompt engineering, and security measures will be crucial for enhancing the system's robustness, performance, and overall user experience.